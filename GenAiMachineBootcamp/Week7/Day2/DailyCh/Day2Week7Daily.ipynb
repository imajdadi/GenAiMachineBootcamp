{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhBxUS54XhFK",
        "outputId": "e9423cb4-190a-49a3-e6d7-de6545da8539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "--2025-03-26 00:04:37--  https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.55, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/30/98/3098b37681411cb3ef2ad6ed32143545199e6b34e74acbd09d9270e1fccc4e70/b0330a2205ca2c7a243d4a67be42b1f49ac66089d5d318970896f9f7291f020e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gemma-3-1b-it.Q8_0.gguf%3B+filename%3D%22gemma-3-1b-it.Q8_0.gguf%22%3B&Expires=1742951077&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjk1MTA3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzMwLzk4LzMwOThiMzc2ODE0MTFjYjNlZjJhZDZlZDMyMTQzNTQ1MTk5ZTZiMzRlNzRhY2JkMDlkOTI3MGUxZmNjYzRlNzAvYjAzMzBhMjIwNWNhMmM3YTI0M2Q0YTY3YmU0MmIxZjQ5YWM2NjA4OWQ1ZDMxODk3MDg5NmY5ZjcyOTFmMDIwZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=gMgLxucz5RTtuOt59IdJMrGjJ1hJG3ivqk1dkDvheNr-VZJxuhb-qylN0LBNrzoLxGl2wHHGwMkvkIombLmRv%7ESKH2nxSsYPbHc%7E708dBBh63FPnR9yKnICq7Ms2C%7Ew9yBc4D7ijJ7Aqpm3YO96qv4dVWncQSGIx%7EEndBEr6UtGB1dVOFSA3trYIbyxuRZtOKLk7QPuztVDY-4QXOdSO9rcU2H%7ED-wbF9BPS9njZjL1svcI0M1cjOaFMzdUt-bf%7EqnVYUcCRYjruu9zdo10JvtsLaiOczz%7EBUVPmY8lw%7Ejcwxdbi0WlTEm9FSNf9YyRBKjRNI1NZS4uLsvtYK1aMiw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-26 00:04:37--  https://cdn-lfs-us-1.hf.co/repos/30/98/3098b37681411cb3ef2ad6ed32143545199e6b34e74acbd09d9270e1fccc4e70/b0330a2205ca2c7a243d4a67be42b1f49ac66089d5d318970896f9f7291f020e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gemma-3-1b-it.Q8_0.gguf%3B+filename%3D%22gemma-3-1b-it.Q8_0.gguf%22%3B&Expires=1742951077&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjk1MTA3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzMwLzk4LzMwOThiMzc2ODE0MTFjYjNlZjJhZDZlZDMyMTQzNTQ1MTk5ZTZiMzRlNzRhY2JkMDlkOTI3MGUxZmNjYzRlNzAvYjAzMzBhMjIwNWNhMmM3YTI0M2Q0YTY3YmU0MmIxZjQ5YWM2NjA4OWQ1ZDMxODk3MDg5NmY5ZjcyOTFmMDIwZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=gMgLxucz5RTtuOt59IdJMrGjJ1hJG3ivqk1dkDvheNr-VZJxuhb-qylN0LBNrzoLxGl2wHHGwMkvkIombLmRv%7ESKH2nxSsYPbHc%7E708dBBh63FPnR9yKnICq7Ms2C%7Ew9yBc4D7ijJ7Aqpm3YO96qv4dVWncQSGIx%7EEndBEr6UtGB1dVOFSA3trYIbyxuRZtOKLk7QPuztVDY-4QXOdSO9rcU2H%7ED-wbF9BPS9njZjL1svcI0M1cjOaFMzdUt-bf%7EqnVYUcCRYjruu9zdo10JvtsLaiOczz%7EBUVPmY8lw%7Ejcwxdbi0WlTEm9FSNf9YyRBKjRNI1NZS4uLsvtYK1aMiw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.249.126.67, 13.249.126.70, 13.249.126.107, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.249.126.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1069306400 (1020M) [binary/octet-stream]\n",
            "Saving to: ‘gemma-3-1b-it-Q8_0.gguf’\n",
            "\n",
            "gemma-3-1b-it-Q8_0. 100%[===================>]   1020M  23.1MB/s    in 18s     \n",
            "\n",
            "2025-03-26 00:04:55 (55.8 MB/s) - ‘gemma-3-1b-it-Q8_0.gguf’ saved [1069306400/1069306400]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python --verbose\n",
        "\n",
        "!wget https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q8_0.gguf -O gemma-3-1b-it-Q8_0.gguf # Gemma 3 1B model Q8 in GGUF format (compatible with Llama)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si5LjIr2f6NO",
        "outputId": "7c2cc627-c542-47eb-8172-84827dfc7022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 47101, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 47101 (delta 56), reused 28 (delta 28), pack-reused 47024 (from 3)\u001b[K\n",
            "Receiving objects: 100% (47101/47101), 99.06 MiB | 17.96 MiB/s, done.\n",
            "Resolving deltas: 100% (33813/33813), done.\n",
            "/content/llama.cpp/llama.cpp\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "\u001b[33mCMake Warning at CMakeLists.txt:107 (message):\n",
            "  LLAMA_CUDA is deprecated and will be removed in the future.\n",
            "\n",
            "  Use GGML_CUDA instead\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:113 (llama_option_depr)\n",
            "\n",
            "\u001b[0m\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "-- CUDA Toolkit found\n",
            "-- Using CUDA architectures: native\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "\u001b[0m-- CUDA host compiler is GNU 11.4.0\n",
            "\u001b[0m\n",
            "-- Including CUDA backend\n",
            "-- Configuring done (14.0s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/llama.cpp/build\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  2%] Built target ggml-base\n",
            "[  2%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  2%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  3%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  3%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  3%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  4%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  4%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  4%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  5%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  5%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  5%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
            "nvcc warning : Cannot find valid GPU for '-arch=native', default arch is used\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!mkdir build\n",
        "!cmake -DLLAMA_CUDA=on -B build\n",
        "!cmake --build build --config Release\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQwoTrh1f_Mn",
        "outputId": "d6a86699-5b90-4b6f-e59d-d8f5afa14045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: git-lfs in /usr/local/lib/python3.11/dist-packages (1.6)\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "fatal: destination path 'CodeLlama-7b-hf' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/codellama/CodeLlama-7b-hf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXytjwtcgH3Q",
        "outputId": "861c11e5-ed16-41df-df53-b800a5586d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/llama.cpp/llama.cpp/scripts/convert-hf-to-gguf.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python llama.cpp/scripts/convert-hf-to-gguf.py \\\n",
        "    --model_dir CodeLlama-7b-hf \\\n",
        "    --output_dir ./ \\\n",
        "    --outtype Q8_0  # Format GGUF quantifié pour optimiser la mémoire\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0bTvjzifTpP"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Charger le modèle GGUF\n",
        "llm = Llama(model_path=\"CodeLlama-7b-Q8_0.gguf\")  # Remplace par ton modèle GGUF\n",
        "\n",
        "# Définir une invite (prompt)\n",
        "prompt = \"Explique-moi le système solaire en quelques phrases.\"\n",
        "\n",
        "# Générer du texte\n",
        "output = llm(prompt, max_tokens=100)\n",
        "print(output[\"choices\"][0][\"text\"])\n",
        "\n",
        "# Génération de texte avec affichage en streaming\n",
        "output_stream = llm(\n",
        "    prompt,\n",
        "    max_tokens=100,\n",
        "    stream=True  # Activation du streaming\n",
        ")\n",
        "\n",
        "# Affichage progressif des tokens générés\n",
        "for output in output_stream:\n",
        "    token = output[\"choices\"][0][\"text\"]\n",
        "    print(token, end=\"\", flush=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
